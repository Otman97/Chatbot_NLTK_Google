{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R3vIOCKNu1Pd"
   },
   "source": [
    "# Simple Chatbot from Scratch in Python using NLTK and search in web\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLuLNKGcu1Pn"
   },
   "source": [
    "## NLP\n",
    "NLP is a way for computers to analyze, understand, and derive meaning from human language in a smart and useful way. By utilizing NLP, developers can organize and structure knowledge to perform tasks such as automatic summarization, translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gI-_oOy2u1Po"
   },
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "RtpYvdGju1Po"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import random\n",
    "import string # to process standard python strings\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "import requests\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUHDhvbMu1Pp"
   },
   "source": [
    "## Downloading and installing NLTK\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PMRCPd3eu1Pq",
    "outputId": "17b6b7e9-024e-466f-8f92-e0a7c1cca039"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECT66F8Au1Pr"
   },
   "source": [
    "### Installing NLTK Packages\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LmjLANmKu1Pr",
    "outputId": "3e946a4b-34a9-49b7-8dc1-e44ddc801f96"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('popular', quiet=True) # for downloading packages\n",
    "#nltk.download('punkt') # first-time use only\n",
    "#nltk.download('wordnet') # first-time use only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B32XyEq9u1Ps"
   },
   "source": [
    "## Reading in the corpus\n",
    "\n",
    "For our example,we will be using the Wikipedia page for chatbots as our corpus. Copy the contents from the page and place it in a text file named ‘chatbot.txt’. However, you can use any corpus of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ED9Nv-4Cv_Rh",
    "outputId": "e35adedf-61a1-4d5a-8923-3284358423b9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "szDxxNgnwLgR"
   },
   "outputs": [],
   "source": [
    "directions='./Chatbot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jSqabooqu1Pt"
   },
   "outputs": [],
   "source": [
    "f=open(directions+'chatbot.txt','r',errors = 'ignore')\n",
    "raw=f.read()\n",
    "raw = raw.lower()# converts to lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXDV1Y68u1Pu"
   },
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7uCKPNJCu1Pu"
   },
   "outputs": [],
   "source": [
    "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
    "word_tokens = nltk.word_tokenize(raw)# converts to list of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIUz27wju1Pu"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "We shall now define a function called LemTokens which will take as input the tokens and return normalized tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TGuDcwUXu1Pv"
   },
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "up_hF5hGu1Pv"
   },
   "source": [
    "## Keyword matching\n",
    "\n",
    "Next, we shall define a function for a greeting by the bot i.e if a user’s input is a greeting, the bot shall return a greeting response.ELIZA uses a simple keyword matching for greetings. We will utilize the same concept here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dI4_cnCAu1Pv"
   },
   "outputs": [],
   "source": [
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "def greeting(sentence):\n",
    " \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEFqjeuXu1Py"
   },
   "source": [
    "## get response\n",
    "To generate a response from our bot for input questions, the concept of document similarity will be used. We define a function response which searches the user’s utterance for one or more known keywords and returns one of several possible responses. If it doesn’t find the input matching any of the keywords, it returns a response:” I am sorry! I don’t understand you”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RT0nQwbCu1Pz"
   },
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        #robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
    "        robo_response=cherche(user_response)\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-RV4nTe1CIv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgS9ncC41DwA"
   },
   "source": [
    "## search  in internet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "S-3KbT-I0ok_"
   },
   "outputs": [],
   "source": [
    "def Gsearch(cherche):\n",
    "\n",
    "\n",
    "  count = 0\n",
    "  url=''\n",
    "  try :\n",
    "      from googlesearch import search\n",
    "  except ImportError:\n",
    "      print(\"No Module named 'google' Found\")\n",
    "  for i in search(query=cherche,tld='co.in',lang='en',num=10,stop=1,pause=2):\n",
    "      count += 1\n",
    "            #print (count)\n",
    "            #print(i + '\\n')\n",
    "      url+=i\n",
    "            #print(url)\n",
    "  return url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ObV_03ZU0oxc"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "# link for extract html data\n",
    "def getdata(url):\n",
    "\tr = requests.get(url)\n",
    "\treturn r.text\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "VCLVYr0v0o1o"
   },
   "outputs": [],
   "source": [
    "#la premier paragraphe\n",
    "\n",
    "def get2_info(htmldata):\n",
    "\n",
    "  soup = BeautifulSoup(htmldata, 'html.parser')\n",
    "  data = ''\n",
    "  for data in soup.find_all(\"p\"):\n",
    "\t  return data.get_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QrtopmzB0o6O",
    "outputId": "d31c3056-37d1-439b-a346-659960df7c08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data are units of information, often numeric, that are collected through observation.[1] In a more technical sense, data are a set of values of qualitative or quantitative variables about one or more persons or objects,[1] while a datum (singular of data) is a single value of a single variable.[2]\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cherche(cherche):\n",
    "\n",
    "  test=Gsearch(cherche)\n",
    "  htmldata = getdata(test)\n",
    "  info=get2_info(htmldata)\n",
    "  return info\n",
    "cherche('what is data?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "fbaQKrbW0pH1"
   },
   "outputs": [],
   "source": [
    "\n",
    "from gtts import gTTS \n",
    "import os\n",
    "from playsound import playsound\n",
    "#text = 'Global warming is the long-term rise in \\n the average temperature of the Earth’s climate system'\n",
    "#language = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2speech(text,language ):\n",
    "    speech = gTTS(text = text, lang = language, slow = False)\n",
    "    if os.path.exists(directions+\"text.mp3\"):\n",
    "        remov=os.remove(directions+\"text.mp3\")\n",
    "        if remov==None:\n",
    "            speech.save(directions+'text.mp3')\n",
    "    else:\n",
    "        speech.save(directions+'text.mp3')\n",
    "    #os.system('start text.mp3')\n",
    "    playsound(directions+\"text.mp3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnu9SI0Nu1Pz"
   },
   "source": [
    "Finally, we will feed the lines that we want our bot to say while starting and ending a conversation depending upon user’s input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JcZEkHzFu1P0",
    "outputId": "7fd590b8-60c7-43a3-9fff-063fb8edc0d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\n",
      "what is chatbot\n",
      "ROBO: design\n",
      "the chatbot design is the process that defines the interaction between the user and the chatbot.the chatbot designer will define the chatbot personality, the questions that will be asked to the users, and the overall interaction.it can be viewed as a subset of the conversational design.\n",
      "bye\n",
      "ROBO: Bye! take care..\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\")\n",
    "while(flag==True):\n",
    "    text=''\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' or user_response=='OK' ):\n",
    "            flag=False\n",
    "            print(\"ROBO: You are welcome..\")\n",
    "            text2speech('You are welcome','en' )\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"ROBO: \"+greeting(user_response))\n",
    "                text=greeting(user_response)\n",
    "                text2speech(text,'en' )\n",
    "            else:\n",
    "                print(\"ROBO: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"ROBO: Bye! take care..\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copie de Chatbot.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
